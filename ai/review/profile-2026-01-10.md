# Performance Profile: hhg Build and Search (2026-01-10)

## Environment

| Component     | Value                        |
| ------------- | ---------------------------- |
| Machine       | Mac M3 Max, 128GB            |
| Model         | gte-modernbert-base          |
| Backend       | MLX (Metal GPU)              |
| Batch Size    | 64                           |
| Test Codebase | ./src (10 files, 112 blocks) |

## Executive Summary

| Operation     | Time     | Bottleneck                     |
| ------------- | -------- | ------------------------------ |
| Build         | 4.5-6.3s | Embedding (85%)                |
| Search        | 2.3s     | Python startup + imports (75%) |
| Search (warm) | ~200ms   | omendb hybrid_search (60%)     |

## Build Operation

### Phase Breakdown

| Phase         | Time    | % of Total | Notes                      |
| ------------- | ------- | ---------- | -------------------------- |
| Scan          | 0.5ms   | <0.1%      | Mojo scanner, blazing fast |
| Extract       | 82.6ms  | 1.3%       | Tree-sitter parallel       |
| Embed         | 1188ms  | 18.8%      | MLX batched inference      |
| Pool overhead | ~2900ms | 46%        | multiprocessing join wait  |
| Imports       | ~1850ms | 29%        | mlx, transformers, etc     |
| DB write      | <50ms   | <1%        | omendb storage             |

### Embedding Analysis

Throughput varies significantly by batch homogeneity:

| Batch Type                | Texts/sec | Notes                    |
| ------------------------- | --------- | ------------------------ |
| Homogeneous (same length) | 2895      | 64 short texts           |
| Heterogeneous (mixed)     | 100-300   | Length bucketing helps   |
| Real data (112 blocks)    | 91-117    | Wide length distribution |

**Text length distribution** (112 blocks):

- Min: 27 chars, Max: 28,480 chars
- Mean: 1,690 chars, Median: 749 chars
- P90: 3,688 chars, P99: 17,872 chars

**Bucketing impact**: Sorting texts by length before embedding improved throughput by ~28% (91 -> 117 texts/sec).

### Hot Path: mlx_embedder.py

```
Location: src/hygrep/mlx_embedder.py:116-154 (_embed_batch)
Cumtime: 938ms per build
Issue: Length bucketing helps but 100-token buckets too coarse
```

Current bucketing groups texts into 100-token (~400 char) buckets. With wide length variation (27-28480 chars), buckets still contain heterogeneous texts causing padding waste.

## Search Operation

### Cold Start (CLI)

| Phase              | Time    | Notes                        |
| ------------------ | ------- | ---------------------------- |
| Python startup     | ~500ms  | Interpreter + base imports   |
| Module imports     | ~1200ms | mlx_embeddings, transformers |
| SemanticIndex init | 70ms    | ContextExtractor (67ms)      |
| Query embedding    | 9ms     | Single text, cached model    |
| omendb search      | 119ms   | Hybrid (vector + BM25)       |
| **Total**          | ~2270ms |                              |

### Warm Search (in-process)

| Phase              | Time   | Notes                                |
| ------------------ | ------ | ------------------------------------ |
| SemanticIndex init | 70ms   | ContextExtractor created each search |
| Query embedding    | 9ms    | Model already loaded                 |
| omendb search      | 119ms  |                                      |
| **Total**          | ~200ms |                                      |

### Hot Path: extractor.py

```
Location: src/hygrep/extractor.py:215 (__init__)
Cumtime: 70ms per search
Issue: ContextExtractor recreated each search, compiles 20+ tree-sitter queries
```

The SemanticIndex creates a new ContextExtractor on every init, even though it's only needed for build operations, not search.

## Optimization Opportunities

### High Impact

| Optimization          | Expected Gain    | Effort | File:Line           |
| --------------------- | ---------------- | ------ | ------------------- |
| Lazy ContextExtractor | -70ms search     | Low    | semantic.py:184     |
| Sort texts by length  | +25% embed speed | Low    | semantic.py:386     |
| Finer length buckets  | +20% embed speed | Low    | mlx_embedder.py:131 |

### Medium Impact

| Optimization           | Expected Gain | Effort | File:Line        |
| ---------------------- | ------------- | ------ | ---------------- |
| Cache compiled queries | -20ms init    | Low    | extractor.py:215 |
| Reduce import time     | -200ms cold   | Medium | Multiple         |
| Process pool reuse     | -50ms build   | Medium | semantic.py:356  |

### Low Priority

| Optimization         | Expected Gain     | Effort | Notes                 |
| -------------------- | ----------------- | ------ | --------------------- |
| Concurrent embedding | +50% large builds | High   | GPU already saturated |
| Daemon mode          | -1500ms cold      | High   | Complexity overhead   |

## Recommendations

### 1. Lazy ContextExtractor (Immediate)

```python
# semantic.py:184
# Current:
self.extractor = ContextExtractor()

# Proposed:
self._extractor = None
@property
def extractor(self):
    if self._extractor is None:
        self._extractor = ContextExtractor()
    return self._extractor
```

**Impact**: -70ms per search (35% improvement on warm search)

### 2. Sort Embeddings by Length (Immediate)

```python
# semantic.py:386 (in embed loop)
# Current:
texts = [b["text"] for b in batch]
embeddings = self.embedder.embed(texts)

# Proposed:
# Sort by length, embed, restore original order
texts_with_idx = [(i, b["text"]) for i, b in enumerate(batch)]
texts_with_idx.sort(key=lambda x: len(x[1]))
sorted_texts = [t for _, t in texts_with_idx]
embeddings_sorted = self.embedder.embed(sorted_texts)
# Restore order
embeddings = np.empty_like(embeddings_sorted)
for new_idx, (orig_idx, _) in enumerate(texts_with_idx):
    embeddings[orig_idx] = embeddings_sorted[new_idx]
```

**Impact**: +25% embedding throughput

### 3. Finer Token Buckets (Easy)

```python
# mlx_embedder.py:131
# Current: bucket_size = 100 tokens (~400 chars)
# Proposed: bucket_size = 50 tokens (~200 chars)
bucket_size = 50
```

**Impact**: +20% embedding throughput (fewer padding tokens)

## Validation Plan

After implementing optimizations:

```bash
# Measure search latency
time pixi run hhg "semantic search" ./src -q  # Target: <1.8s (from 2.3s)

# Measure build throughput
time pixi run hhg build ./src -q  # Target: <4.0s (from 4.5s)

# Measure warm search (Python)
python -c "...; index.search('query')"  # Target: <130ms (from 200ms)
```

## Appendix: Raw Profile Data

### Build cProfile (top functions by cumtime)

```
ncalls  tottime  cumtime  filename:lineno(function)
     1    0.000    2.936  multiprocessing/pool.py:738(__exit__)
     1    0.000    1.823  hygrep/semantic.py:1(<module>)
     1    0.000    1.760  hygrep/embedder.py:1(<module>)
     2    0.001    0.938  hygrep/mlx_embedder.py:116(_embed_batch)
    26    0.001    0.936  hygrep/mlx_embedder.py:89(_embed_batch_safe)
```

### Search cProfile (top functions by cumtime)

```
ncalls  tottime  cumtime  filename:lineno(function)
     1    0.000    0.133  hygrep/semantic.py:435(search)
     1    0.119    0.119  omendb.search_hybrid
     1    0.070    0.070  hygrep/extractor.py:215(__init__)
     1    0.000    0.009  hygrep/mlx_embedder.py:179(embed_one)
```
