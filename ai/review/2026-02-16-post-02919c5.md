# Code Review: Changes since 02919c5

**Date:** 2026-02-16
**Scope:** 10 files, ~600 lines changed (including new mcp.rs at 364 lines)
**Commits:** 04301f2..1759e93 (8 commits: multi-model -> single model simplification, MCP server, fixes)

## Summary

| Severity | Count |
| -------- | ----- |
| ERROR    | 1     |
| WARN     | 2     |
| NIT      | 0     |

## Critical (must fix)

**[ERROR] src/embedder/onnx.rs:112-113 -- embed_query uses document tokenizer instead of query tokenizer**

`embed_query` calls `self.embed_batch(&[text])` which internally uses `self.tokenizer.encode_documents()` (max_length=512). The `query_tokenizer` (max_length=256) and `encode_query()` method exist in `tokenizer.rs` but are never called. ColBERT-style models expect different tokenization for queries vs documents.

```rust
// Current: uses encode_documents (doc tokenizer, 512 tokens)
fn embed_query(&self, text: &str) -> Result<Array2<f32>> {
    let result = self.embed_batch(&[text])?;  // calls encode_documents
    ...
}
```

Fix: Add a dedicated query path that uses `encode_query`:

```rust
fn embed_query(&self, text: &str) -> Result<Array2<f32>> {
    let encoding = self.tokenizer.encode_query(text)?;
    // Run single-item inference with query encoding
    let seq_len = encoding.get_ids().len();
    let input_ids: Vec<i64> = encoding.get_ids().iter().map(|&id| id as i64).collect();
    let attention_mask: Vec<i64> = encoding.get_attention_mask().iter().map(|&m| m as i64).collect();
    // ... (same inference logic as embed_batch but for single query)
}
```

Or refactor `embed_batch` to accept a flag/enum for doc vs query tokenization.

## Important (should fix)

**[WARN] src/cli/build.rs:102-107 + 111-118 -- Duplicate subdir index cleanup**

Subdir indexes are cleaned up inside the "no index exists" branch (lines 102-107), then cleaned up again unconditionally at lines 111-118. On a fresh build, the "Cleaned up N subdir indexes" message prints twice. The second `remove_dir_all` calls fail silently (errors ignored), but the duplicate message is user-visible.

Fix: Remove the inner cleanup (lines 102-107) and rely on the outer cleanup (lines 111-118), or add an `else` guard so only one runs:

```rust
} else {
    build_index(&build_path, quiet)?;
    // Don't clean here, the code at lines 111-118 handles it
}
```

**[WARN] src/cli/build.rs:95 -- Stale TODO comment**

```rust
// TODO: implement merge_from_subdir for VectorStore
// For now, just build fresh and clean up subdirs after
```

This is an empty block with a TODO. Per project conventions, no TODOs.

## Strengths

1. **Clean simplification.** The multi-model -> single-model refactor removed ~50 lines of configuration/registry code while keeping the `ModelConfig` struct for future extensibility. The `batch_size` parameter is properly internalized.

2. **Centralized model downloads.** Moving download logic from `onnx.rs` and `tokenizer.rs` into `embedder::download_model_files` eliminates duplicate HF Hub API creation and gives a single place for download error messages.

3. **Atomic writes.** Both `manifest.rs:save()` and `mcp.rs:install_claude_code()` now use tmp-file-then-rename for crash safety. Good defensive practice.

4. **MCP server implementation.** Clean JSON-RPC 2.0 handling with proper parse error responses, notification handling (skip messages without id), auto-update before search, and capped `num_results` (`.min(100)`).

5. **Dead code removal.** The "different model" check in build error handling was correctly identified as dead code after removing the model validation from manifest loading (since there's now only one model).

## Verdict

**Approve with required fix.** The query tokenizer bug (ERROR) should be fixed -- it affects search quality since ColBERT query encoding differs from document encoding. The duplicate cleanup (WARN) is a cosmetic/UX issue that should also be addressed. Overall the changes are well-structured and the MCP server is solid.
