{
  "project": "tk",
  "ref": "pbf0",
  "title": "Implement quality benchmark (CodeSearchNet + MRR)",
  "description": "Quality eval script in bench/quality.py. Corpus: CodeSearchNet Python test split (~14k functions). Queries: docstrings as NL queries (500 sampled). Metrics: MRR@10, Recall@1/5/10. HF dataset source broken (loading scripts deprecated) - researcher agent launched to find working mirror (a5f878a78fe5ab15d). Check CoIR-Retrieval, Nan-Do, direct jsonl download. og JSON output fields: file, name, type, line, score, content. Match gold by basename of file path (corpus/{idx:06d}.py). Also implement CoSQA if available.",
  "status": "done",
  "priority": 2,
  "labels": [],
  "assignees": [],
  "parent": null,
  "blocked_by": [],
  "estimate": null,
  "due_date": null,
  "logs": [
    {
      "ts": "2026-02-22T10:31:38.059Z",
      "msg": "First run: MRR=0.0082, Recall@1=0, Recall@5=0, Recall@10=0.08 (2000 corpus, 100 queries, seed=42). 16x over random but absolute numbers low. Root cause: corpus heavy with Azure SDK boilerplate functions that are structurally identical â€” model can't distinguish them. Script works correctly. Scores all clustered near -12 to -13, no discrimination. Shorter queries or more diverse corpus needed for meaningful numbers."
    }
  ],
  "created_at": "2026-02-22T09:42:32.549Z",
  "updated_at": "2026-02-22T10:31:38.081Z",
  "completed_at": "2026-02-22T10:31:38.081Z",
  "external": {}
}